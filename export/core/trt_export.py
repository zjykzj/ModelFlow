# -*- coding: utf-8 -*-

"""
@Time    : 2025/7/9 16:46
@File    : trt_exporter.py
@Author  : zj
@Description: 
"""

# core/trt_exporter.py

import subprocess
import os
import onnx


def get_input_profile(onnx_path, batch_size):
    """
    ä» ONNX æ¨¡å‹ä¸­æå–è¾“å…¥ä¿¡æ¯ï¼Œç”Ÿæˆ trtexec æ‰€éœ€çš„ shape å‚æ•°ã€‚

    è¿”å›ç¤ºä¾‹:
        input_name = 'input'
        opt_shape = 'input:1x3x224x224'
        max_shape = 'input:8x3x224x224'
    """
    model = onnx.load(onnx_path)
    input_tensor = model.graph.input[0]
    input_name = input_tensor.name

    # è·å–è¾“å…¥ç»´åº¦
    dims = input_tensor.type.tensor_type.shape.dim
    dynamic_batch = False

    input_shape = []
    for i, dim in enumerate(dims):
        if dim.HasField("dim_value"):
            val = dim.dim_value
        elif dim.HasField("dim_param"):
            val = 1  # åŠ¨æ€ç»´åº¦ä½¿ç”¨é»˜è®¤å€¼ä½œä¸º opt å€¼
            dynamic_batch = (i == 0)  # ç¬¬ä¸€ç»´æ˜¯å¦æ˜¯åŠ¨æ€ batchï¼Ÿ
        else:
            val = 1

        input_shape.append(val)

    # å›ºå®š batch å¤§å°
    input_shape[0] = 1
    max_shape_list = input_shape.copy()
    max_shape_list[0] = batch_size

    def shape_to_str(shape):
        return "x".join(map(str, shape))

    opt_shape_str = f"{input_name}:{shape_to_str(input_shape)}"
    max_shape_str = f"{input_name}:{shape_to_str(max_shape_list)}"

    return input_name, opt_shape_str, max_shape_str, dynamic_batch


def convert_onnx_to_tensorrt(
        onnx_path: str,
        engine_path: str,
        fp16: bool = False,
        batch_size: int = None,  # åŠ¨æ€æ‰¹é‡å¤§å°ï¼ŒNone è¡¨ç¤ºå›ºå®š ONNX åŸå§‹å€¼
        workspace: int = 4096,  # å•ä½ MBï¼Œé»˜è®¤ 4096 MB = 4GB
        avg_runs: int = 100,  # æ€§èƒ½æµ‹è¯•è¿è¡Œæ¬¡æ•°
        verbose: bool = False
):
    """
    ä½¿ç”¨ trtexec å°† ONNX è½¬æ¢ä¸º TensorRT å¼•æ“ï¼Œæ”¯æŒåŠ¨æ€è¾“å…¥å½¢çŠ¶å’Œæ€§èƒ½è°ƒä¼˜å‚æ•°ã€‚
    """
    if not os.path.exists(onnx_path):
        raise FileNotFoundError(f"ONNX model file not found: {onnx_path}")

    cmd = [
        "trtexec",
        f"--onnx={onnx_path}",
        f"--saveEngine={engine_path}",
        "--explicitBatch"
    ]

    # ç²¾åº¦è®¾ç½®
    if fp16:
        cmd += ["--fp16", "--inputIOFormats=fp16:chw", "--outputIOFormats=fp16:chw"]

    # æ˜¾å­˜é™åˆ¶
    cmd += [f"--workspace={workspace}"]

    if batch_size is not None and batch_size > 1:
        # è¾“å…¥å½¢çŠ¶è®¾ç½®
        input_name, opt_shape, max_shape, _ = get_input_profile(onnx_path, batch_size)
        cmd += [
            f"--optShapes={opt_shape}",
            f"--maxShapes={max_shape}",
            f"--minShapes={opt_shape}"
        ]
    else:
        print("â„¹ï¸ ä½¿ç”¨ ONNX æ¨¡å‹ä¸­å®šä¹‰çš„è¾“å…¥å½¢çŠ¶ï¼ˆæœªæŒ‡å®šåŠ¨æ€èŒƒå›´ï¼‰")

    # æ€§èƒ½è¯„ä¼°å‚æ•°
    cmd += [f"--avgRuns={avg_runs}"]

    if verbose:
        print("ğŸš€ Running trtexec command:")
        print(" ".join(cmd))

    print("ğŸ”„ Converting ONNX to TensorRT engine...")
    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)

    # æ— è®ºæˆåŠŸä¸å¦ï¼Œéƒ½æ‰“å° trtexec çš„å®Œæ•´è¾“å‡ºæ—¥å¿—
    print("ğŸ“‹ TensorRT conversion log:")
    print(result.stdout)

    if result.returncode != 0:
        print("âŒ Error during TensorRT conversion:")
        print(result.stdout)
        raise RuntimeError("TensorRT engine conversion failed.")
    else:
        print(f"âœ… TensorRT engine saved at: {engine_path}")
