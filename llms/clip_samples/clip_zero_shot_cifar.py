# -*- coding: utf-8 -*-

"""
@Time    : 2026/2/8 17:16
@File    : cifar_zero_shot.py
@Author  : zj
@Description:

ä½¿ç”¨åŸå§‹æ¨¡æ¿è¿›è¡Œè¯„ä¼°

root@autodl-container-00e345b2a0-c853a801:~/zj/ModelFlow/llms/clip_samples# python3 eval_cifar.py
ğŸš€ Using device: cuda
ğŸ§  Loading CLIP model: ViT-B/32 ...
âœ… Model loaded and set to eval mode.
ğŸ“‚ Loading dataset: CIFAR10 (test set) ...
ğŸ“Š Dataset size: 10000 samples | Classes: 10
ğŸ”¤ Encoding text prompts...
âœ… Encoded 10 text features.
ğŸ” Starting zero-shot evaluation...
Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:05<00:00, 28.80it/s]

============================================================
ğŸ¯ Zero-Shot Classification Results
   Model:       ViT-B/32
   Dataset:     CIFAR10
   Accuracy:    88.8000%  (8880/10000)
============================================================

root@autodl-container-00e345b2a0-c853a801:~/zj/ModelFlow/llms/clip_samples# python3 eval_cifar.py --dataset cifar100
ğŸš€ Using device: cuda
ğŸ§  Loading CLIP model: ViT-B/32 ...
âœ… Model loaded and set to eval mode.
ğŸ“‚ Loading dataset: CIFAR100 (test set) ...
ğŸ“Š Dataset size: 10000 samples | Classes: 100
ğŸ”¤ Encoding text prompts...
âœ… Encoded 100 text features.
ğŸ” Starting zero-shot evaluation...
Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:05<00:00, 29.37it/s]

============================================================
ğŸ¯ Zero-Shot Classification Results
   Model:       ViT-B/32
   Dataset:     CIFAR100
   Accuracy:    61.7000%  (6170/10000)
============================================================

ä½¿ç”¨ä¼˜åŒ–æ¨¡æ¿è¿›è¡Œè¯„ä¼°

root@autodl-container-00e345b2a0-c853a801:~/zj/ModelFlow/llms/clip_samples# python3 eval_cifar.py
ğŸš€ Using device: cuda
ğŸ§  Loading CLIP model: ViT-B/32 ...
âœ… Model loaded and set to eval mode.
ğŸ“‚ Loading dataset: CIFAR10 (test set) ...
ğŸ“Š Dataset size: 10000 samples | Classes: 10
ğŸ”¤ Encoding text prompts with ensemble templates...
âœ… Encoded text features using 20 templates.
ğŸ” Starting zero-shot evaluation...
Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:05<00:00, 30.13it/s]

============================================================
ğŸ¯ Zero-Shot Classification Results
   Model:       ViT-B/32
   Dataset:     CIFAR10
   Accuracy:    89.5200%  (8952/10000)
============================================================
root@autodl-container-00e345b2a0-c853a801:~/zj/ModelFlow/llms/clip_samples#
root@autodl-container-00e345b2a0-c853a801:~/zj/ModelFlow/llms/clip_samples#
root@autodl-container-00e345b2a0-c853a801:~/zj/ModelFlow/llms/clip_samples#
root@autodl-container-00e345b2a0-c853a801:~/zj/ModelFlow/llms/clip_samples# python3 eval_cifar.py --dataset cifar100
ğŸš€ Using device: cuda
ğŸ§  Loading CLIP model: ViT-B/32 ...
âœ… Model loaded and set to eval mode.
ğŸ“‚ Loading dataset: CIFAR100 (test set) ...
ğŸ“Š Dataset size: 10000 samples | Classes: 100
ğŸ”¤ Encoding text prompts with ensemble templates...
âœ… Encoded text features using 20 templates.
ğŸ” Starting zero-shot evaluation...
Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:04<00:00, 32.10it/s]

============================================================
ğŸ¯ Zero-Shot Classification Results
   Model:       ViT-B/32
   Dataset:     CIFAR100
   Accuracy:    63.9600%  (6396/10000)
============================================================

"""

import os
import argparse
import torch
import clip
from tqdm import tqdm

from torchvision.datasets import CIFAR10, CIFAR100
from torch.utils.data import DataLoader


def get_dataset(dataset_name, preprocess):
    """æ ¹æ®åç§°è¿”å›å¯¹åº”çš„æ•°æ®é›†å’Œç±»åˆ«åˆ—è¡¨"""
    root = os.path.expanduser("~/.cache")
    if dataset_name.lower() == "cifar10":
        dataset = CIFAR10(root=root, train=False, download=True, transform=preprocess)
        classes = [
            "airplane", "automobile", "bird", "cat", "deer",
            "dog", "frog", "horse", "ship", "truck"
        ]
    elif dataset_name.lower() == "cifar100":
        dataset = CIFAR100(root=root, train=False, download=True, transform=preprocess)
        # CIFAR-100 å®˜æ–¹ç»†ç²’åº¦ç±»åˆ«ï¼ˆæŒ‰é¡ºåºï¼‰
        classes = [
            'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',
            'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',
            'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',
            'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',
            'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',
            'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',
            'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',
            'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',
            'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',
            'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',
            'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',
            'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',
            'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',
            'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'
        ]
    else:
        raise ValueError(f"Unsupported dataset: {dataset_name}. Choose 'cifar10' or 'cifar100'.")
    return dataset, classes


def get_templates():
    """è¿”å›ç”¨äºé›†æˆçš„æ‰‹å·¥æ¨¡æ¿åˆ—è¡¨"""
    return [
        'a photo of a {}.',
        'a blurry photo of a {}.',
        'a black and white photo of a {}.',
        'a low contrast photo of a {}.',
        'a high contrast photo of a {}.',
        'a bad photo of a {}.',
        'a good photo of a {}.',
        'a photo of a small {}.',
        'a photo of a large {}.',
        'a photo of a {} with sharp focus.',
        'a photo of many {}s.',
        'a close-up photo of a {}.',
        'a cropped photo of a {}.',
        'a bright photo of a {}.',
        'a dark photo of a {}.',
        'a photo of my {}.',
        'i love my {}!',
        'a plastic {}.',
        'a toy {}.',
        'a cartoon {}.'
    ]


def main():
    parser = argparse.ArgumentParser(description="Zero-shot evaluation of CLIP on CIFAR datasets.")
    parser.add_argument("--dataset", type=str, default="cifar10",
                        choices=["cifar10", "cifar100"],
                        help="Dataset to evaluate on (default: cifar10)")
    parser.add_argument("--model", type=str, default="ViT-B/32",
                        help="CLIP model name (default: ViT-B/32)")
    parser.add_argument("--batch_size", type=int, default=64,
                        help="Batch size for inference (default: 64)")
    parser.add_argument("--num_workers", type=int, default=4,
                        help="Number of workers for data loading (default: 4)")
    args = parser.parse_args()

    # ----------------------------
    # 1. è®¾ç½®è®¾å¤‡ä¸æ¨¡å‹
    # ----------------------------
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ Using device: {device}")
    print(f"ğŸ§  Loading CLIP model: {args.model} ...")
    model, preprocess = clip.load(args.model, device=device)
    model.eval()
    print("âœ… Model loaded and set to eval mode.")

    # ----------------------------
    # 2. åŠ è½½æ•°æ®é›†ä¸ç±»åˆ«
    # ----------------------------
    print(f"ğŸ“‚ Loading dataset: {args.dataset.upper()} (test set) ...")
    test_dataset, class_names = get_dataset(args.dataset, preprocess)
    num_classes = len(class_names)
    print(f"ğŸ“Š Dataset size: {len(test_dataset)} samples | Classes: {num_classes}")

    test_loader = DataLoader(
        test_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True if device == "cuda" else False
    )

    # ----------------------------
    # 3. ç¼–ç æ–‡æœ¬ promptsï¼ˆåªåšä¸€æ¬¡ï¼‰
    # ----------------------------
    # print("ğŸ”¤ Encoding text prompts...")
    # with torch.no_grad():
    #     text_inputs = clip.tokenize([f"a photo of a {c}" for c in class_names]).to(device)
    #     text_features = model.encode_text(text_inputs)
    #     text_features /= text_features.norm(dim=-1, keepdim=True)
    # print(f"âœ… Encoded {num_classes} text features.")

    print("ğŸ”¤ Encoding text prompts with ensemble templates...")
    templates = get_templates()
    all_text_features = []

    with torch.no_grad():
        for template in templates:
            # ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆå¸¦æ¨¡æ¿çš„å¥å­
            sentences = [template.format(c) for c in class_names]
            text_tokens = clip.tokenize(sentences).to(device)
            text_feats = model.encode_text(text_tokens)
            text_feats /= text_feats.norm(dim=-1, keepdim=True)
            all_text_features.append(text_feats)

        # å¯¹æ‰€æœ‰æ¨¡æ¿çš„ç‰¹å¾å–å¹³å‡ â†’ æ¯ä¸ªç±»åˆ«ä¸€ä¸ªæ›´é²æ£’çš„æ–‡æœ¬è¡¨ç¤º
        text_features = torch.stack(all_text_features).mean(dim=0)
        text_features /= text_features.norm(dim=-1, keepdim=True)  # å†æ¬¡å½’ä¸€åŒ–

    print(f"âœ… Encoded text features using {len(templates)} templates.")

    # ----------------------------
    # 4. æ‰¹é‡æ¨ç†ä¸è¯„ä¼°
    # ----------------------------
    correct = 0
    total = 0
    logit_scale = model.logit_scale.exp()

    print("ğŸ” Starting zero-shot evaluation...")
    with torch.no_grad():
        for batch_idx, (images, labels) in enumerate(tqdm(test_loader, desc="Inference")):
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            # Encode images
            image_features = model.encode_image(images)
            image_features /= image_features.norm(dim=-1, keepdim=True)

            # Compute logits
            logits = logit_scale * (image_features @ text_features.T)  # [B, K]

            # Predict
            preds = logits.argmax(dim=1)
            batch_correct = (preds == labels).sum().item()
            batch_total = labels.size(0)

            correct += batch_correct
            total += batch_total

            # å¯é€‰ï¼šæ‰“å°æ¯ä¸ª batch çš„å‡†ç¡®ç‡ï¼ˆè°ƒè¯•ç”¨ï¼Œå¯æ³¨é‡Šï¼‰
            # batch_acc = batch_correct / batch_total
            # print(f"  Batch {batch_idx + 1}: Acc = {batch_acc:.4f} ({batch_correct}/{batch_total})")

    accuracy = correct / total
    print("\n" + "=" * 60)
    print(f"ğŸ¯ Zero-Shot Classification Results")
    print(f"   Model:       {args.model}")
    print(f"   Dataset:     {args.dataset.upper()}")
    print(f"   Accuracy:    {accuracy:.4%}  ({correct}/{total})")
    print("=" * 60)


if __name__ == "__main__":
    main()
