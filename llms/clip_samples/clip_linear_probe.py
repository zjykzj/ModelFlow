# -*- coding: utf-8 -*-

"""
@Time    : 2026/2/8 17:50
@File    : clip_linear_probe.py
@Author  : zj
@Description: 
"""

import os
import argparse
import torch
import clip
import numpy as np
from tqdm import tqdm

from torchvision.datasets import CIFAR10, CIFAR100
from torch.utils.data import DataLoader
from sklearn.linear_model import LogisticRegression


def get_dataset(dataset_name, preprocess, train=True):
    """åŠ è½½ CIFAR-10 æˆ– CIFAR-100 æ•°æ®é›†"""
    root = os.path.expanduser("~/.cache")
    if dataset_name.lower() == "cifar10":
        dataset = CIFAR10(root=root, train=train, download=True, transform=preprocess)
        num_classes = 10
    elif dataset_name.lower() == "cifar100":
        dataset = CIFAR100(root=root, train=train, download=True, transform=preprocess)
        num_classes = 100
    else:
        raise ValueError(f"Unsupported dataset: {dataset_name}. Use 'cifar10' or 'cifar100'.")
    return dataset, num_classes


def extract_features(model, dataloader, device):
    """ä» dataloader ä¸­æå–æ‰€æœ‰å›¾åƒçš„ CLIP ç‰¹å¾ï¼ˆL2 å½’ä¸€åŒ–ï¼‰"""
    all_features = []
    all_labels = []
    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc="Extracting features"):
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            # æå–å¹¶å½’ä¸€åŒ–å›¾åƒç‰¹å¾
            image_features = model.encode_image(images)
            image_features /= image_features.norm(dim=-1, keepdim=True)

            all_features.append(image_features.cpu().numpy())
            all_labels.append(labels.cpu().numpy())
    return np.vstack(all_features), np.concatenate(all_labels)


def main():
    parser = argparse.ArgumentParser(description="Linear Probe evaluation of CLIP on CIFAR datasets.")
    parser.add_argument("--dataset", type=str, default="cifar10",
                        choices=["cifar10", "cifar100"],
                        help="Dataset to evaluate on (default: cifar10)")
    parser.add_argument("--model", type=str, default="ViT-B/32",
                        help="CLIP model name (default: ViT-B/32)")
    parser.add_argument("--batch_size", type=int, default=64,
                        help="Batch size for feature extraction (default: 64)")
    parser.add_argument("--num_workers", type=int, default=4,
                        help="Number of workers for data loading (default: 4)")
    args = parser.parse_args()

    # ----------------------------
    # 1. è®¾ç½®è®¾å¤‡ä¸æ¨¡å‹
    # ----------------------------
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ Using device: {device}")
    print(f"ğŸ§  Loading CLIP model: {args.model} ...")
    model, preprocess = clip.load(args.model, device=device)
    model.eval()
    for param in model.parameters():
        param.requires_grad_(False)  # å†»ç»“æ•´ä¸ªæ¨¡å‹
    print("âœ… Model loaded and frozen.")

    # ----------------------------
    # 2. åŠ è½½è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    # ----------------------------
    print(f"ğŸ“‚ Loading {args.dataset.upper()} training set...")
    train_dataset, num_classes = get_dataset(args.dataset, preprocess, train=True)
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=(device == "cuda")
    )

    print(f"ğŸ“‚ Loading {args.dataset.upper()} test set...")
    test_dataset, _ = get_dataset(args.dataset, preprocess, train=False)
    test_loader = DataLoader(
        test_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=(device == "cuda")
    )

    # ----------------------------
    # 3. æå–è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç‰¹å¾
    # ----------------------------
    print("ğŸ” Extracting features from training set...")
    train_features, train_labels = extract_features(model, train_loader, device)

    print("ğŸ” Extracting features from test set...")
    test_features, test_labels = extract_features(model, test_loader, device)

    print(f"ğŸ“Š Feature shape - Train: {train_features.shape}, Test: {test_features.shape}")

    # ----------------------------
    # 4. è®­ç»ƒçº¿æ€§åˆ†ç±»å™¨ï¼ˆLogistic Regressionï¼‰
    # ----------------------------
    print("ğŸ› ï¸ Training Logistic Regression classifier...")
    classifier = LogisticRegression(
        random_state=42,
        max_iter=1000,
        C=1.0,  # å¯å°è¯•è°ƒæ•´æ­£åˆ™å¼ºåº¦ï¼ˆå¦‚ C=0.1, 1.0, 10.0ï¼‰
        # solver='lbfgs',  # é€‚ç”¨äºä¸­å°è§„æ¨¡å¤šåˆ†ç±»
        # multi_class='multinomial'
    )
    classifier.fit(train_features, train_labels)
    print("âœ… Classifier training completed.")

    # ----------------------------
    # 5. è¯„ä¼°æµ‹è¯•é›†å‡†ç¡®ç‡
    # ----------------------------
    print("ğŸ§ª Evaluating on test set...")
    test_preds = classifier.predict(test_features)
    accuracy = (test_preds == test_labels).mean()

    print("\n" + "=" * 60)
    print(f"ğŸ¯ Linear Probe Classification Results")
    print(f"   Model:       {args.model}")
    print(f"   Dataset:     {args.dataset.upper()}")
    print(f"   Train Size:  {len(train_labels)}")
    print(f"   Test Size:   {len(test_labels)}")
    print(f"   Accuracy:    {accuracy:.4%}  ({int(accuracy * len(test_labels))}/{len(test_labels)})")
    print("=" * 60)


if __name__ == "__main__":
    main()
